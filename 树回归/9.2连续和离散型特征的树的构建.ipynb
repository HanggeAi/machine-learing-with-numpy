{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 程序清单9-1 CART算法的实现代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def loadDataSet(filename):\n",
    "    \"\"\"数据加载函数,但只返回整个,无单独的label\"\"\"\n",
    "    dataMat=[]\n",
    "    fr=open(filename)\n",
    "    for line in fr.readlines():\n",
    "        currentLine=line.strip().split(\"\\t\")\n",
    "        fltLine=list(map(float,currentLine))\n",
    "        dataMat.append(fltLine)\n",
    "        \n",
    "    return dataMat\n",
    "\n",
    "\n",
    "def binSplitDataSet(dataSet,feature,value):\n",
    "    \"\"\"选择dataset的某个特征feature,将整个数据集按照feature与value的大小关系进行二分\"\"\"\n",
    "    mat0=dataSet[np.nonzero(dataSet[:,feature]>value)[0],:]\n",
    "    mat1=dataSet[np.nonzero(dataSet[:,feature]<=value)[0],:]\n",
    "    return mat0,mat1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试一下上面这俩函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testMat=np.arange(12).reshape(4,3)\n",
    "\n",
    "testMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 6,  7,  8],\n",
       "        [ 9, 10, 11]]),\n",
       " array([[0, 1, 2],\n",
       "        [3, 4, 5]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binSplitDataSet(testMat,1,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 将CART算法用于回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1 构建树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 程序清单9-2 回归树的切分函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regLeaf(dataSet):\n",
    "    \"\"\"负责生成叶结点的函数\n",
    "    当函数chooseBestSplit确定不再对数据进行切分时,就会调用该函数,\n",
    "    来得到叶结点的模型\n",
    "    回归树中,该模型其实就是目标变量的均值\n",
    "    \"\"\"\n",
    "    return np.mean(dataSet[:,-1])\n",
    "\n",
    "\n",
    "def regErr(dataSet):\n",
    "    \"\"\"返回给定数据上的目标变量的平方误差,可理解为计算分类问题的基尼系数(实际上这是基于最小二乘偏差)\"\"\"\n",
    "    return np.var(dataSet[:,-1])*dataSet.shape[0]\n",
    "\n",
    "\n",
    "def chooseBestSplit(dataSet,leafType=regLeaf,errType=regErr,ops=(1,4)):\n",
    "    \"\"\"找到数据集的最佳二分切分方式\n",
    "    如果找不到,则返回None并且同时调用creatTree来产生叶结点,叶结点也返回None\n",
    "    ops中,第一个元素为容许下降的误差值,第二个元素是切分的最小样本数\n",
    "    注意该函数有三个提前结束条件\"\"\"\n",
    "    m,n=dataSet.shape\n",
    "    S=errType(dataSet)    # 总体混乱程度\n",
    "    tolS=ops[0]    # 能够容忍的最小方差变化量\n",
    "    tolN=ops[1]    # 能够容忍的最小矩阵样本数量\n",
    "    if len(set(dataSet[:,-1].T.tolist()[0]))==1:    # 如果数据集中的目标值全部相等，则创建叶结点并退出\n",
    "        return None,leafType(dataSet)\n",
    "    \n",
    "    # 初始化最优方差，最优特征，最优特征值\n",
    "    bestS=np.inf\n",
    "    bestIndex=0    # 最优特征的索引\n",
    "    bestValue=0\n",
    "    for featIndex in range(n-1):    # 遍历每个特征\n",
    "        for splitVal in set(dataSet[:,featIndex].T.tolist()[0]):\n",
    "            mat0,mat1=binSplitDataSet(dataSet,featIndex,splitVal)\n",
    "            if mat0.shape[0]<tolN or mat1.shape[0]<tolN:    # 划分得到的子数据集的尺寸过小 则重新划分\n",
    "                continue\n",
    "            # 子数据集的方差和\n",
    "            newS=errType(mat0)+errType(mat1)    # 新的混乱程度\n",
    "            \n",
    "            # 更新\n",
    "            if newS<bestS:\n",
    "                bestS=newS\n",
    "                bestIndex=featIndex\n",
    "                bestValue=splitVal\n",
    "    \n",
    "    if S-bestS<tolS:    # 如果误差减少不大，则退出\n",
    "        return None,leafType(dataSet)\n",
    "    \n",
    "    # 使用最优特征和特征值切分\n",
    "    mat0,mat1=binSplitDataSet(dataSet,bestIndex,bestValue)\n",
    "    if mat0.shape[0]<tolN or mat1.shape[0]<tolN:\n",
    "        return None,leafType(dataSet)\n",
    "    \n",
    "    return bestIndex,bestValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n",
    "    \"\"\"\n",
    "    常见决策树的递归函数\n",
    "    leafType:给出建立叶结点的函数\n",
    "    errType:给出误差计算函数\n",
    "    \"\"\"\n",
    "    feat, val = chooseBestSplit(\n",
    "        dataSet, leafType, errType, ops)    # 选择使得信息增益最大的特征及其分界值val\n",
    "    \n",
    "    if feat==None:    # 递归到没有特征可以划分了，则返回该特征对应的最佳划分值(这里不可以if not None 代替该语句！)\n",
    "        return val\n",
    "    retTree = {}\n",
    "    retTree['spInd'] = feat\n",
    "    retTree['spVal'] = val\n",
    "    # 根据最优特征及其划分值,划分子集\n",
    "    lSet, rSet = binSplitDataSet(dataSet, feat, val)\n",
    "    retTree['left'] = createTree(\n",
    "        lSet, leafType, errType, ops)     # 利用划分后的左子集,递归创建左子树\n",
    "    retTree['right'] = createTree(rSet, leafType, errType, ops)\n",
    "\n",
    "    return retTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath='D:\\\\机器学习实战代码\\\\machinelearninginaction\\\\Ch09\\\\ex00.txt'\n",
    "myDat=loadDataSet(dataPath)\n",
    "myMat=np.mat(myDat)\n",
    "\n",
    "a=createTree(myMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spInd': 0,\n",
       " 'spVal': 0.48813,\n",
       " 'left': 1.0180967672413792,\n",
       " 'right': -0.04465028571428572}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 换一个数据集试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spInd': 1,\n",
       " 'spVal': 0.441815,\n",
       " 'left': {'spInd': 1,\n",
       "  'spVal': 0.808177,\n",
       "  'left': 4.581648499999999,\n",
       "  'right': {'spInd': 1,\n",
       "   'spVal': 0.621523,\n",
       "   'left': 4.233747156250001,\n",
       "   'right': 3.912047575757576}},\n",
       " 'right': {'spInd': 1,\n",
       "  'spVal': 0.212575,\n",
       "  'left': 3.563709000000001,\n",
       "  'right': 3.1889351956521743}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPath1='D:\\\\机器学习实战代码\\\\machinelearninginaction\\\\Ch08\\\\ex0.txt'\n",
    "myDat=loadDataSet(dataPath1)\n",
    "myMat=np.mat(myDat)\n",
    "\n",
    "a=createTree(myMat)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 后剪枝\n",
    "#### 程序清单9-3 回归树剪枝函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTree(obj):\n",
    "    \"\"\"判断输入数据是否为一棵树(这里为字典)\"\"\"\n",
    "    return isinstance(obj,dict)\n",
    "\n",
    "\n",
    "def getMean(tree):\n",
    "    if isTree(tree['left']):\n",
    "        tree['left']=getMean(tree['left'])\n",
    "    if isTree(tree['right']):\n",
    "        tree['right']=getMean(tree['right'])\n",
    "    return (tree['left']+tree['right'])/2.0\n",
    "\n",
    "\n",
    "def prune(tree,testData):\n",
    "    if testData.shape[0]==0:\n",
    "        return getMean(tree)    # 如无测试数据，则对树进行塌方处理\n",
    "    if (isTree(tree['left'])) or (isTree(tree['right'])):    # 如果有一个孩子是树的话，则考察当前节点的划分数据集\n",
    "        lSet,rSet=binSplitDataSet(testData,tree['spInd'],tree['spVal'])\n",
    "    if isTree(tree['left']):\n",
    "        tree['left']=prune(tree['left'],lSet)\n",
    "    if isTree(tree['right']):\n",
    "        tree['right']=prune(tree['right'],rSet)\n",
    "    if not isTree(tree['left']) and not isTree(tree['right']):\n",
    "        lSet,rSet=binSplitDataSet(testData,tree['spInd'],tree['spVal'])\n",
    "        # 计算合并前的混乱程度\n",
    "        errorNoMerge=np.sum(np.power(lSet[:,-1]-tree['left'],2))+\\\n",
    "            np.sum(np.power(rSet[:,-1]-tree['right'],2))\n",
    "        # 计算合并后的混乱程度\n",
    "        treeMean=(tree['left']+tree['right'])/2.0\n",
    "        errorMerge=np.sum(np.power(testData[:,-1]-treeMean,2))\n",
    "        \n",
    "        if errorMerge<errorNoMerge:\n",
    "            print('merging...')\n",
    "            return treeMean\n",
    "        else:\n",
    "            return tree\n",
    "    \n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 模型树\n",
    "#### 之前的决策树是将叶结点设置为常数，而模型树是将叶结点设置为线性模型，这样更加精准\n",
    "---\n",
    "#### 程序清单9-4 模型树的叶结点生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearSolve(dataSet):\n",
    "    m,n=dataSet.shape\n",
    "    x=np.mat(np.ones((m,n)))\n",
    "    y=np.mat(np.ones((m,1)))\n",
    "    x[:,1:n]=dataSet[:,:n-1]\n",
    "    y=dataSet[:,-1]\n",
    "    \n",
    "    xTx=x.T*x\n",
    "    if np.linalg.det(xTx)==0:\n",
    "        print('the det of xTx is 0,None has been returned.')\n",
    "        return np.array([0,0,0])\n",
    "    ws=xTx.I*(x.T*y)\n",
    "    return ws,x,y\n",
    "\n",
    "\n",
    "def modelLeaf(dataSet):\n",
    "    ws,X,y=linearSolve(dataSet)\n",
    "    return ws\n",
    "\n",
    "\n",
    "def modelErr(dataSet):\n",
    "    \"\"\"误差计算函数,基于线性模型的预测误差\"\"\"\n",
    "    ws,X,y=linearSolve(dataSet)\n",
    "    yHat=X*ws\n",
    "    return np.sum(np.power(y-yHat,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath1='D:\\\\机器学习实战代码\\\\machinelearninginaction\\\\Ch09\\\\exp2.txt'\n",
    "myDat=loadDataSet(dataPath1)\n",
    "myMat=np.mat(myDat)\n",
    "\n",
    "modelTree=createTree(myMat,modelLeaf,modelErr,ops=(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spInd': 0,\n",
       " 'spVal': 0.285477,\n",
       " 'left': matrix([[1.69855694e-03],\n",
       "         [1.19647739e+01]]),\n",
       " 'right': matrix([[3.46877936],\n",
       "         [1.18521743]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 示例：树回归与标准回顾的比较\n",
    "#### 程序清单9-5 用树回归进行预测的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regTreeEval(model,inDat):\n",
    "    return float(model)\n",
    "\n",
    "\n",
    "def modelTreeEval(model,inDat):\n",
    "    n=inDat.shape[1]\n",
    "    # 构造X\n",
    "    X=np.mat(np.ones((1,n+1)))\n",
    "    X[:,1:n+1]=inDat\n",
    "    return float(X*model)\n",
    "\n",
    "\n",
    "def treeForeCast(tree,inDat,modelEval=regTreeEval):\n",
    "    if not isTree(tree):    # 如果tree不是一棵树,而只是一个叶子结点\n",
    "        return regTreeEval(tree,inDat)\n",
    "    # 如果tree是树，则需要递归预测\n",
    "    if inDat[tree['spInd']]>tree['spVal']:    # 输入数据的特征值与树的分裂界限值比较，决定进入左子树还是右子树\n",
    "        if isTree(tree['left']):    # 如果tree的左子节点为树，则递归预测\n",
    "            return treeForeCast(tree['left'],inDat,modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['left'],inDat)\n",
    "    else:\n",
    "        if isTree(tree['right']):\n",
    "            return treeForeCast(tree['right'],inDat,modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['right'],inDat)\n",
    "        \n",
    "        \n",
    "def createForeCast(tree,testData,modelEval=regTreeEval):\n",
    "    m=len(testData)\n",
    "    yHat=np.mat(np.zeros((m,1)))\n",
    "    for i in range(m):\n",
    "        yHat[i,0]=treeForeCast(tree,testData[i],modelEval)    # 每次预测一条数据\n",
    "    return yHat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9640852318222145"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path='D:\\\\机器学习实战代码\\\\machinelearninginaction\\\\Ch09\\\\bikeSpeedVsIq_train.txt'\n",
    "test_data_path='D:\\\\机器学习实战代码\\\\machinelearninginaction\\\\Ch09\\\\bikeSpeedVsIq_test.txt'\n",
    "\n",
    "# 获取矩阵\n",
    "trainMat=np.mat(loadDataSet(train_data_path))\n",
    "testMat=np.mat(loadDataSet(test_data_path))\n",
    "\n",
    "myTree=createTree(trainMat,ops=(1,20))\n",
    "yHat=createForeCast(myTree,testMat[:,0])    # 使用第一列预测\n",
    "\n",
    "np.corrcoef(yHat,testMat[:,1],rowvar=0)[0,1]    # R**2计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.96408523],\n",
       "       [0.96408523, 1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(yHat,testMat[:,1],rowvar=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 看看线性回归的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[37.58916794],\n",
       "        [ 6.18978355]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws,X,y=linearSolve(trainMat,)\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9639387703067179"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(testMat.shape[1]):\n",
    "    yHat[i] = testMat[i,0]*ws[1,0]+ws[0,0]\n",
    "    \n",
    "np.corrcoef(yHat,testMat[:,1],rowvar=0)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 差不多其实。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69bb06a9fb724e3616bb36eae2c2891ed4de586fe76c0b350d4e65619dfe458e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
