{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算给定数据集的信息熵，基于$H=-\\sum_{i=1}^n p(x_i)log_2 p(x_i)$\n",
    "#### 程序清单3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calShannonEnt(dataSet):\n",
    "    \"\"\"dataSet:二维array类型或者二维列表类型,包含最后一列分类属性\"\"\"\n",
    "    num=len(dataSet)    # 总样本个数\n",
    "    labelCounts={}    # key:属性名称，value:拥有该属性的样本所占的数量,len(labelCounts)即为分类数|y|\n",
    "    for row in dataSet:    # 遍历每一行\n",
    "        currentLabel=row[-1]\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel]=0    # 初始化键值对\n",
    "        labelCounts[currentLabel]+=1\n",
    "    \n",
    "    # 计算香农熵\n",
    "    shannonEnt=0.0    # 初始化\n",
    "    for key in labelCounts.keys():\n",
    "        prob=labelCounts[key]/num    # pk\n",
    "        shannonEnt+=prob*np.log2(prob)\n",
    "        \n",
    "    return -shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createDataSet():\n",
    "    dataSet = [\n",
    "        [1,1,'yes'],\n",
    "        [1,1,'yes'],\n",
    "        [1,0,'no'],\n",
    "        [0,1,'no'],\n",
    "        [0,1,'no'],\n",
    "    ]\n",
    "    labels=['no surfacing','flippers']\n",
    "    return dataSet, labels\n",
    "\n",
    "\n",
    "myData=createDataSet()[0]\n",
    "calShannonEnt(myData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 程序清单3-2 按照给定特征划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splitDataSet(dataSet,axis,value):\n",
    "    \"\"\"挑选出axis所对应的特征为value的数据项\"\"\"\n",
    "    retDataSet=[]    # 防止更改原列表，新建一个list\n",
    "    for row in dataSet:\n",
    "        if row[axis]==value:\n",
    "            reducedFeatVec=row[:axis]\n",
    "            reducedFeatVec.extend(row[axis+1:])    # 丢弃row[axis]\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "            \n",
    "    return retDataSet\n",
    "\n",
    "\n",
    "myData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'yes'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(myData,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据信息增益，选择最好的划分特征，基于：\n",
    "#### $Gain(D,a)=Ent(D)-\\sum_{v=1}^{V} \\frac{|D^v|}{|D|}Ent(D^v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    \"\"\"分别计算每个特征划分后的Gain,以此选取最优特征作为划份特征\n",
    "    返回最优特征对应的列索引\"\"\"\n",
    "    numFeatures = len(dataSet[0])-1    # 注意，最后一列不是特征！！！（'yes','no'这一列！）\n",
    "    baseEntropy=calShannonEnt(dataSet)    # Ent(D)\n",
    "    \n",
    "    bestInfoGain=0\n",
    "    bestFeature=-1    # 初始化\n",
    "    \n",
    "    for i in range(numFeatures):    # 遍历每个特征，使得每个特征作为一个划分特征\n",
    "        featureList=[row[i] for row in dataSet]\n",
    "        uniqueVals=list(set(featureList))    # unique之,得到v\n",
    "        newEntropy=0    # 记录公式中\\sum项\n",
    "        for value in uniqueVals:\n",
    "            subDataSet=splitDataSet(dataSet,i,value)\n",
    "            # 被除数需为浮点数\n",
    "            newEntropy += calShannonEnt(subDataSet) * \\\n",
    "                (len(subDataSet)/float(len(dataSet)))\n",
    "\n",
    "        infoGain=baseEntropy-newEntropy    # Gain\n",
    "        \n",
    "        if infoGain>bestInfoGain:\n",
    "            bestInfoGain=infoGain    # 将最大的infoGain作为bestInfoGain\n",
    "            bestFeature=i    # 且标记该infoGain对应的特征\n",
    "    return bestFeature\n",
    "\n",
    "\n",
    "chooseBestFeatureToSplit(myData)\n",
    "# 结果表明，应首先选择第0个特征作为 划分特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个函数，类似于np.unique(arr,return_counts=True)\n",
    "import operator    # operator.func返回一个可调用对象，如：s=operator.add,s(1,1)->2\n",
    "\n",
    "\n",
    "def majorCnt(classList):\n",
    "    \"\"\"返回classList中种类对应数量最多的种类\"\"\"\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    \n",
    "    sortedClassList=sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassList[0][0]\n",
    "\n",
    "\n",
    "majorCnt([0,0,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 递归构建决策树\n",
    "#### 程序清单3-4 创建数的函数代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def creatTree(dataSet,labels):\n",
    "    \"\"\"\n",
    "    dataSet: dataSet\n",
    "    labels: dataSet中每个样本对应的标签,算法本身不需要该变量，\n",
    "    将其也作为输入的原因是为了给出数据明确的含义\n",
    "    \n",
    "    return: 以字典数据结构保存的决策树\n",
    "    \"\"\"\n",
    "    classList=[row[-1] for row in dataSet]    # 所有类的标签\n",
    "    \n",
    "    # 递归结束的条件1：classList中的类别完全相同，则停止划分\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]    # 叶子结点\n",
    "    \n",
    "    # 递归结束的基本条件2：遍历完所有的划分属性，但是仍无法将数据集划分成包含唯一一个类别的组\n",
    "    # 此时返回classList中数量最多的类别作为最终类别\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorCnt(classList)\n",
    "    \n",
    "    # 递归调用\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet)    # 选择最好的特征用于划分子集\n",
    "    bestFeatLabel=labels[bestFeat]    # 将索引值转化为有意义的类别（特征）名称\n",
    "    \n",
    "    myTree={bestFeatLabel:{}}    # 使用字典表示树\n",
    "    del(labels[bestFeat])    # 用过的特征就不在用了\n",
    "    \n",
    "    featValues=[row[bestFeat] for row in dataSet]    # 最优特征对应的取值w,为一列\n",
    "    uniqueVals=set(featValues)    # unique一下，为使用该特征进一步划分做准备\n",
    "    \n",
    "    for value in uniqueVals:\n",
    "        sublabels=labels[:]    # 浅拷贝，此时的labels也是缩减过的,之前的del()函数\n",
    "        myTree[bestFeatLabel][value]=creatTree(splitDataSet(dataSet,bestFeat,value),sublabels)    # splitDataSet\n",
    "        # splitDataSet的过程中，减小了问题规模（dataSet的列数得到了缩减）\n",
    "        # 只有在字典的键名为bestFeatLabel下面，才递归创建决策树\n",
    "    return myTree\n",
    "\n",
    "\n",
    "myDat,labels=createDataSet()\n",
    "creatTree(myDat,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 示例：使用决策树预测隐形眼镜模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = open('D:\\\\机器学习实战代码\\\\machinelearninginaction\\\\Ch03\\\\lenses.txt')\n",
    "lenses=[row.strip().split('\\t') for row in fr.readlines()]\n",
    "lensesLabels=['age','prescript','astigmatic','tearRate']\n",
    "lensesTree=creatTree(lenses,lensesLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tearRate': {'reduced': 'no lenses',\n",
       "  'normal': {'astigmatic': {'yes': {'prescript': {'myope': 'hard',\n",
       "      'hyper': {'age': {'pre': 'no lenses',\n",
       "        'young': 'hard',\n",
       "        'presbyopic': 'no lenses'}}}},\n",
       "    'no': {'age': {'pre': 'soft',\n",
       "      'young': 'soft',\n",
       "      'presbyopic': {'prescript': {'myope': 'no lenses',\n",
       "        'hyper': 'soft'}}}}}}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lensesTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69bb06a9fb724e3616bb36eae2c2891ed4de586fe76c0b350d4e65619dfe458e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
