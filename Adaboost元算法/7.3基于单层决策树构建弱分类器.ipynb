{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据的函数\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def loadSimpData():\n",
    "    datMat=np.matrix([\n",
    "        [1.,2.1],\n",
    "        [2.,1.1],\n",
    "        [1.3,1.],\n",
    "        [1.,1.],\n",
    "        [2.,1.]\n",
    "    ])\n",
    "    classLabels=[1.,1.,-1.,-1.,1.]\n",
    "    return datMat,classLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单层决策树的构建\n",
    "#### 是通过暴力穷举生成对当前数据集权重的最优决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'thresh': 1.3, 'ineq': 'lt'},\n",
       " matrix([[0.2]]),\n",
       " array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n",
    "    \"\"\"\n",
    "    似乎这是一个基于单特征的分类器\n",
    "    dimen:特征,维度,第几列\n",
    "    threshVal:阈值\n",
    "    threshIneq:阈值不等号:('lt','gt')\n",
    "    \"\"\"\n",
    "    retArray = np.ones((dataMatrix.shape[0], 1))    # 要返回的预测值\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:, dimen] > threshVal] = -1.0\n",
    "    return retArray\n",
    "\n",
    "\n",
    "def buildStump(dataArr, classLabels, D):\n",
    "    \"\"\"\n",
    "    构造单层决策树\n",
    "    D:数据的权重向量\n",
    "    return: 最优的单层决策树\n",
    "    \"\"\"\n",
    "    # 初始化\n",
    "    dataMatrix = np.mat(dataArr)\n",
    "    labelMat = np.mat(classLabels).T\n",
    "    m, n = dataMatrix.shape\n",
    "    numsteps = 10.0    # 用于在特征的所有可能值上进行遍历\n",
    "    bestStump = {}   # 用于保存最优决策树\n",
    "    bestClassEst = np.mat(np.zeros((m, 1)))\n",
    "    minError = np.inf   # 初始化最小误差\n",
    "\n",
    "    for i in range(n):    # 遍历每个特征\n",
    "        rangeMin = dataMatrix[:, i].min()    # 特征的最小值\n",
    "        rangeMax = dataMatrix[:, i].max()\n",
    "        stepSize = (rangeMax-rangeMin)/numsteps    # 步幅\n",
    "        for j in range(-1, int(numsteps)+1):\n",
    "            for inequal in ['lt', 'gt']:    # 对每个不等号\n",
    "                # 对阈值从rangeMin遍历到rangeMax\n",
    "                threshVal = rangeMin+float(j)*stepSize\n",
    "                predictedVals = stumpClassify(\n",
    "                    dataMatrix, i, threshVal, inequal)\n",
    "                errArr = np.mat(np.ones((m, 1)))    # 误差，列向量，初始化为全部预测错误，故全1\n",
    "                errArr[predictedVals == labelMat] = 0   # 预测对的置为0\n",
    "                # 计算加权错误率\n",
    "                weightedError = D.T*errArr    # 这个函数中，样本的权重D用来计算误差，\n",
    "                # 由于错误的样本对应的权重更大，且对应的errArr为1，所以会导致weightedError更大，\n",
    "                # 使得下面的if语句不易进行，从而进行更多的循环，进行更细致的划分策略。\n",
    "\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError    # 更新最小误差\n",
    "                    bestClassEst = predictedVals.copy()   # 更新最优预测结果。\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClassEst\n",
    "\n",
    "\n",
    "D = np.mat(np.ones((5, 1))/5)    # 初始化权重\n",
    "datMat, classLabels = loadSimpData()\n",
    "buildStump(datMat, classLabels, D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 完整AdaBoost算法的实现\n",
    "#### 程序清单7-2 基于单层决策树的AdaBoost训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果:  [[-1.  1. -1. -1.  1.]]\n",
      "总错误率： 0.2\n",
      "预测结果:  [[ 1.  1. -1. -1. -1.]]\n",
      "总错误率： 0.2\n",
      "预测结果:  [[ 1.  1. -1. -1.  1.]]\n",
      "总错误率： 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453},\n",
       " {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565},\n",
       " {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adaBoostTrainDS(dataArr, classLabels, numIter=40):\n",
    "    \"\"\"\n",
    "    numIter: number of iterations,也是弱学习器的个数\n",
    "    返回弱分类器列表及其对应的权重\n",
    "    \"\"\"\n",
    "    # 初始化\n",
    "    weakClassArr = []    # 弱学习器列表\n",
    "    m = dataArr.shape[0]\n",
    "    D = np.mat(np.ones((m, 1)))/m    # 初始化各个样本的权重\n",
    "    aggClassEst = np.mat(np.zeros((m, 1)))    # 当前弱学习器的加权预测值\n",
    "\n",
    "    for i in range(numIter):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n",
    "        # print('各个样本的权重: ', D.T)\n",
    "        # 计算弱学习器的权重alpha同时防止除以0溢出\n",
    "        alpha = float(0.5*np.log((1-error)/max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "        # 更新各个样本的权重\n",
    "        expon = np.multiply(-1*alpha*np.mat(classLabels).T, classEst)\n",
    "        D = np.multiply(D, np.exp(expon))\n",
    "        D = D/D.sum()    # 归一\n",
    "\n",
    "        aggClassEst += alpha*classEst    # 加权预测值\n",
    "        print('预测结果: ', np.sign(aggClassEst).T)\n",
    "        aggErrors = np.multiply(np.sign(aggClassEst)!=np.mat(classLabels).T,np.ones((m,1)))    # 010100011的形式\n",
    "        errorRate=aggErrors.sum()/m\n",
    "        print('总错误率：',errorRate)\n",
    "        if errorRate==0:\n",
    "            break    # 提前训练完成，则直接提前退出\n",
    "    return weakClassArr\n",
    "\n",
    "\n",
    "classfierArr=adaBoostTrainDS(datMat, classLabels,10)\n",
    "classfierArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 测试算法：基于AdaBoost的分类\n",
    "#### 程序清单7-3 AdaBoost分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个弱分类器的预测结果: [[-1.]]\n",
      "本次迭代的结果（未符号函数作用时） [[-0.69314718]]\n",
      "————————————————————————————————————————————————————\n",
      "第1个弱分类器的预测结果: [[-1.]]\n",
      "本次迭代的结果（未符号函数作用时） [[-1.66610226]]\n",
      "————————————————————————————————————————————————————\n",
      "第2个弱分类器的预测结果: [[-1.]]\n",
      "本次迭代的结果（未符号函数作用时） [[-2.56198199]]\n",
      "————————————————————————————————————————————————————\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adaClassify(datToClass, classifyArr):\n",
    "    \"\"\"\n",
    "    datToClass:要分类的样本\n",
    "    classifier:训练好的ada分类器\n",
    "    return:+-1 标签\n",
    "    \"\"\"\n",
    "    dataMatrix=np.mat(datToClass)\n",
    "    m=dataMatrix.shape[0]\n",
    "    aggClassEst=np.mat(np.zeros((m,1)))    # 累计预测结果（加权）\n",
    "    for i in range(len(classifyArr)):\n",
    "        classEst=stumpClassify(dataMatrix,\n",
    "                               dimen=classifyArr[i]['dim'],\n",
    "                               threshVal=classifyArr[i]['thresh'],\n",
    "                               threshIneq=classifyArr[i]['ineq'])\n",
    "        aggClassEst+=classEst*classfierArr[i]['alpha']\n",
    "        print('第%d个弱分类器的预测结果:'%i,classEst)\n",
    "        print('本次迭代的结果（未符号函数作用时）',aggClassEst)\n",
    "        print('————————————————————————————————————————————————————')\n",
    "    return np.sign(aggClassEst)\n",
    "\n",
    "\n",
    "adaClassify([0,0],classfierArr)\n",
    "\n",
    "# 由aggClassEst可以看出，随着迭代次数的进行，aggClassEst的值愈发趋向更大的负值，即分类结果越来越强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 示例：在一个难数据集上应用AdaBoost\n",
    "#### 使用病马数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果:  [[-1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.\n",
      "   1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.2842809364548495\n",
      "预测结果:  [[-1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.\n",
      "   1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.2842809364548495\n",
      "预测结果:  [[-1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1. -1.\n",
      "   1.  1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.\n",
      "   1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.24749163879598662\n",
      "预测结果:  [[-1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.\n",
      "   1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1. -1.  1.  1.  1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.24749163879598662\n",
      "预测结果:  [[-1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.\n",
      "   1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1. -1.  1.  1.  1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.25418060200668896\n",
      "预测结果:  [[-1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.\n",
      "   1. -1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.\n",
      "   1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1. -1.  1.  1.  1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.2408026755852843\n",
      "预测结果:  [[-1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.\n",
      "   1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.2408026755852843\n",
      "预测结果:  [[-1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1. -1.\n",
      "  -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1. -1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.\n",
      "  -1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  -1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.22073578595317725\n",
      "预测结果:  [[-1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      "  -1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.\n",
      "   1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.24749163879598662\n",
      "预测结果:  [[-1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1. -1.\n",
      "  -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "   1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1.\n",
      "   1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      "   1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.\n",
      "   1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.\n",
      "  -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  -1.  1.  1.  1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1.\n",
      "   1.  1. -1. -1.  1.  1.  1. -1. -1.  1.  1.]]\n",
      "总错误率： 0.23076923076923078\n"
     ]
    }
   ],
   "source": [
    "def loadDataSet(fileName):\n",
    "    numFeat=len(open(fileName).readline().split('\\t'))\n",
    "    dataMat=[]\n",
    "    labelMat=[]\n",
    "    fr=open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr=[]\n",
    "        curLine=line.strip().split('\\t')\n",
    "        for i in range(numFeat-1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return np.matrix(dataMat),labelMat\n",
    "\n",
    "\n",
    "dataPath='D:\\\\机器学习实战代码\\\\machinelearninginaction\\\\Ch07\\\\horseColicTraining2.txt'\n",
    "train_dat,train_label=loadDataSet(dataPath)\n",
    "\n",
    "\n",
    "# 训练\n",
    "classifierArray=adaBoostTrainDS(train_dat,train_label,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69bb06a9fb724e3616bb36eae2c2891ed4de586fe76c0b350d4e65619dfe458e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
